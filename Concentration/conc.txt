Documents:
(1) https://www.researchgate.net/publication/41781177_Concentration_Inequalities
(2) http://www.stat.rice.edu/~jrojo/PASI/lectures/TyronCMarticle.pdf
(3) https://www.hse.ru/data/2016/11/24/1113029206/Concentration%20inequalities.pdf
(4) http://84.89.132.1/~lugosi/anu.pdf

ğ”¼(X) = âˆ«t p(t) dt
     = t âˆ«p(t)dt |(0,âˆ) - âˆ« âˆ«(0,t') p(t)dt dt'
     = 



    /---
   /----
  /-----
 /------
*------->t
0

    /|||
   /||||
  /|||||
 /||||||
*------->t
0

âˆ«â„™{X â‰¥ t}dt = âˆ«t â„™(t) dt = ğ”¼{X}

â„™{X â‰¥ t} is decreasing
So area in rectangle t Ã— â„™{X â‰¥ t} islways bigger than total integral.


 \
  \
   \
+--+\  â„™{X â‰¥ t}
|  | \
|  |  \
+--+----
0  t

?? +ve t, f(t) >= 0, âˆ« f(t)=1
1-âˆ«f(t)dt <= âˆ« t f(t)dt / t

Markov inequality
-----------------
â„™[Xâ‰¥t] â‰¤ ğ”¼[X]/t

Ï• monotonic increasing
â„™{X â‰¥ t} = â„™{Ï•(X) â‰¥ Ï•(t)} (trivial}
         â‰¤ ğ”¼Ï•(X)/Ï•(t)     (treat Ï•(X) as just some random variable)

--

Eg. Ï•(x) = xÂ²

Chebyshev Inequality
--------------------
P{|X-ğ”¼X| â‰¥ t} = â„™{|X-ğ”¼X|Â² â‰¥ tÂ²}
              â‰¤ ğ”¼[|X-ğ”¼X|Â²]/tÂ²
              = Var{X}/tÂ²
--

Also
P{|X-ğ”¼X| â‰¥ t} â‰¤ ğ”¼[|X-ğ”¼X|áµ–]/táµ–

â„™{X â‰¥ t} â‰¤ ğ”¼exp(sX)/exp(st)

--

â„™[ Xâ‰¥t ] = â„™[exp[sX] â‰¥ exp[st]]
Markov inequality:

Chernoff bounds
---------------
Pr[X â‰¥ t] â‰¤ ğ”¼[exp[sX]]/exp[st]
Choose s to get tightest bound

--

S_n = âˆ‘(i=1..n) Xáµ¢ - independent Xáµ¢

â„™{|S_n-ğ”¼S_n| â‰¥ t} â‰¤ Var{S_n}/tÂ² = âˆ‘(i=1..n) Var{Xáµ¢}/tÂ²

ÏƒÂ² = (1/n)âˆ‘Var(Xáµ¢)

â„™{|(1/n) âˆ‘Xáµ¢-ğ”¼Xáµ¢| â‰¥ Îµ} â‰¤ ÏƒÂ²/(nÎµÂ²)   (trivial)

--

â„™{S_n-ğ”¼S_n â‰¥ t} â‰¤ exp(-st) ğ”¼[ exp(sâˆ‘(Xáµ¢-ğ”¼Xáµ¢)) ]
                = exp(-st) âˆ ğ”¼[ exp(s(Xáµ¢-ğ”¼Xáµ¢)) ]    Inequality (1)

--

Random vectors
--------------
Norm of random vector.
Let X = [Xâ‚ Xâ‚‚ â€¦ Xâ‚ƒ]áµ—
Gaussian, mean 0, covariance matrix P
Does â€–Xâ€–â‚‚Â² concentrate around mean?

MGF
---

ğ”¼[exp[Â±sâ€–Xâ€–â‚‚Â²] = âˆ« dx 1/âˆš(2Ï€ det P) exp(Â±sxÂ·x)exp(-Â½xÂ·Pâ»Â¹)
               = âˆ« dx 1/âˆš(2Ï€ det P) exp(-Â½xÂ·(Pâ»Â¹âˆ“2sI)x)
               = âˆ« dx âˆš(2Ï€ det(Pâ»Â¹âˆ“2sI)â»Â¹)/âˆš(2Ï€ det P) 
               = âˆ« dx âˆš(det(Pâ»Â¹âˆ“2sI)â»Â¹)/âˆš(det P) 
               = âˆ« dx 1/âˆš(det(Iâˆ“2sP))

Eg. special case P=I (So â€–Xâ€–â‚‚Â² ~ Ï‡_nÂ²)
               = (1-2s)^(-n/2)

Special case: X ~ ğ’© (0,ÏƒÂ²I)
Expected norm: ğ”¼[â€–Xâ€–â‚‚Â²] = âˆ‘ğ”¼[Xáµ¢Â²] = n Var(Xâ‚) = nÏƒÂ²

Chernoff bound:
â„™[ â€–Xâ€–â‚‚Â² â‰¥ (1+Îµ)ğ”¼[â€–Xâ€–â‚‚Â²] ] â‰¤ min (s>0) ğ”¼[exp[sâ€–Xâ€–â‚‚Â²]/exp(s(1+Îµ)nÏƒÂ²)
                           = min (s>0) (1-2sÏƒÂ²)^(-n/2) exp(-s(1+Îµ)nÏƒÂ²)


min f(s) = (1-2sÏƒÂ²)^(-n/2) exp(-s(1+Îµ)nÏƒÂ²)

min f(s) = G(s)exp(-As)
f'(s) = G'(s)exp(-As) -AG(s)exp(-As)
Solve: A = G'(s)/G(s)
       A = (log G)'(s)

G(s) = (1-2sÏƒÂ²)^(-n/2)
A = (1+Îµ)nÏƒÂ²
log G(s) = (-n/2) log(1-2ÏƒÂ²s)
(log G(s))' = nÏƒÂ²/(1-2ÏƒÂ²s) = (1+Îµ)nÏƒÂ²
1-2ÏƒÂ²s = nÏƒÂ²/((1+Îµ)nÏƒÂ²)
1-2ÏƒÂ²s = 1/(1+Îµ)
1-1/(1+Îµ) = 2ÏƒÂ²s
Îµ/(1+Îµ) = 2ÏƒÂ²s
s = Îµ/(2ÏƒÂ²(1+Îµ))

Substitute back in:
(1-2sÏƒÂ²)^(-n/2) â†’ (1-2ÎµÏƒÂ²/(2ÏƒÂ²(1+Îµ)))^(-n/2)
               = (1-Îµ/(1+Îµ))^(-n/2)
               = (1+Îµ)^(n/2)
exp(-s(1+Îµ)nÏƒÂ²) â†’ exp(-Îµ(1+Îµ)nÏƒÂ²/(2ÏƒÂ²(1+Îµ)))
                = exp(-Îµn/2)

So â„™[ â€–Xâ€–â‚‚Â² â‰¥ (1+Îµ)ğ”¼[â€–Xâ€–â‚‚Â²] ] â‰¤ ((1+Îµ)exp(-Îµ))^(n/2)
                              = (exp(-Îµ+log(1+Îµ)))^(n/2)

(
log(1+Îµ) â‰¤ Îµ-ÎµÂ²/2+ÎµÂ³/3
L(0) = 0, R(0) = 0
L'(Îµ) = 1/(1+Îµ)
R'(Îµ) = 1-Îµ+ÎµÂ²
L'(0) = 1
R'(0) = 1
L''(Îµ) = -1/(1+Îµ)Â²
R''(Îµ) = -1+2Îµ
L''(0) = -1
R''(0) = -1
Lâ½Â³â¾(Îµ) = 2/(1+Îµ)Â³
Râ½Â³â¾(Îµ) = 2
Lâ½Â³â¾(0) = 2
Râ½Â³â¾(0) = 2
Lâ½â´â¾(Îµ) < Râ½â´â¾ for small Îµ (down to Îµ = -1)
âˆ´ L(Îµ) â‰¤ R(Îµ) for all small Îµ

So (1+Îµ)exp(-Îµ) â‰¤ exp(-(ÎµÂ²/2-ÎµÂ³/3))

So â„™[ â€–Xâ€–â‚‚Â² â‰¥ (1+Îµ)ğ”¼[â€–Xâ€–â‚‚Â²] ] â‰¤ (exp(-(ÎµÂ²/2-ÎµÂ³/3)))^(n/2)

Also ÎµÂ²/6 â‰¥ ÎµÂ³/3 for Îµ â‰¤ Â½
So ÎµÂ²/2-ÎµÂ³/3 â‰¥ ÎµÂ²/3 for 0 < Îµ < Â½

So â„™[ â€–Xâ€–â‚‚Â² â‰¥ (1+Îµ)ğ”¼[â€–Xâ€–â‚‚Â²] ] â‰¤ exp(-ÎµÂ²n/6) 

Similarly ÎµÂ²/2 â‰¤ ÎµÂ²/2-ÎµÂ³/3 for Îµ â‰¤ 0

So â„™[ â€–Xâ€–â‚‚Â² â‰¤ (1-Îµ)ğ”¼[â€–Xâ€–â‚‚Â²] ] â‰¤ exp(-ÎµÂ²n/4)  

--
Johnson-Lindenstrauss
---------------------

Given Îµ > 0, and integer N.
Let m â‰¥ mâ‚€ = O(log(N)/ÎµÂ²)

For all sets P of N points in â„â¿,
there exists Ïˆ:â„â¿ â†’ â„áµ such that for all u,vâˆˆâ„™

(1-Îµ)â€–u-vâ€–Â² â‰¤ â€–Ïˆ(u)-Ïˆ(v)â€–Â² â‰¤ (1+Îµ)â€–u-vâ€–Â²


Choose

Ïˆ(x) = Ax where 

    [aâ‚â‚  aâ‚â‚‚  â€¦ aâ‚_n ]
A = [ â‹®    â‹®   â‹±   â‹®  ]
    [a_mâ‚ a_mâ‚‚ â€¦ a_m_n]

aáµ¢â±¼ ~ ğ’© (0, 1/m) independent

nC2 distances to consider

Fix x. Consider y = Ax.

yáµ¢ = âˆ‘â±¼ aáµ¢â±¼xâ±¼
ğ”¼yáµ¢ = âˆ‘ xâ±¼ ğ”¼aáµ¢â±¼ = 0
Var yáµ¢ = âˆ‘ xâ±¼Â² Var aáµ¢â±¼ = â€–xâ€–â‚‚Â²/m

yáµ¢ ~ ğ’© (0, â€–xâ€–â‚‚Â²/m), independent.

ğ”¼[â€–Axâ€–â‚‚Â²] = ğ”¼[â€–yâ€–â‚‚Â²] = m/m â€–xâ€–â‚‚Â²

So â„™[not (1-Îµ)â€–xâ€–â‚‚Â² â‰¤ â€–Axâ€–â‚‚Â² â‰¤ (1+Îµ)â€–xâ€–â‚‚Â²] â‰¤ 2exp(-ÎµÂ²m/6)

So probability of failing for NC2 vectors is â‰¤ 2(NC2)exp(-ÎµÂ²m/6)
â‰¤ 2(NÂ²/2)exp(-ÎµÂ²m/6)
= exp(-ÎµÂ²m/6+2log N)   (my constant disagrees with notes, doesn't change result)

Small for m > O(log N/min(Îµ,0.5)Â²)

Should come back here to look at matrix trace estimationâ€¦
--


Hoeffding's Inequality (not proved here)

If:
X lies in [a,b].
ğ”¼[X] = 0.

Then:
ğ”¼[exp(sX)] â‰¤ exp(sÂ²(b-a)Â²)/8                        Inequality (2)

--

Combine (1) and (2):

Let S = âˆ‘(báµ¢-aáµ¢)Â²

RHS of (1) is
A = exp(-st+sÂ²S/8)

Best bound: minimize (S/8)sÂ²-st
Diff w.r.t. s, set = 0
(S/4)s = t
s = 4t/S

Substitute back in: A = exp(-2tÂ²/S)

âˆ´ â„™{S_n-ğ”¼S_n â‰¥ t} â‰¤ exp(-2tÂ²/âˆ‘(báµ¢-aáµ¢)Â²)
(Hoeffding inequality)

--

Suppose ğ”¼Xáµ¢ = 0.
Define Ïƒáµ¢Â² = ğ”¼[Xáµ¢Â²]
       Fáµ¢ = ğ”¼[Ïˆ(sXáµ¢)] = âˆ‘(t=2..âˆ) s^(r-2)ğ”¼[Xáµ¢Ê³]/(r!Ïƒáµ¢Â²)
       Ïˆ(x) = exp(x)-x-1

Ïˆ(x) â‰¤ xÂ²/2 for xâ‰¤0.
(
Define f(x) = exp(-x)-x+Â½xÂ²

f'(0) = 0 and xâ‰¥0 â‡’ f''(x)â‰¤0 
So xâ‰¥0 â‡’ f'(x)â‰¤0

f(0) = 0 and xâ‰¥0 â‡’ f'(x)â‰¤0 
So xâ‰¥0 â‡’ f(x)â‰¤0

So Ïˆ(x) â‰¤ xÂ²/2 for xâ‰¤0)

sâ‰¥0 and xâˆˆ[0,1] â‡’ Ïˆ(sx) â‰¤ xÂ²Ïˆ(s)
(
 Ïˆ(sx) = sÂ²xÂ²/2! + sÂ³xÂ³/3! + â€¦
xÂ²Ïˆ(s) = xÂ²(sÂ²/2! + sÂ³/3! + â€¦)
       = sÂ²xÂ²/2! + sÂ³xÂ²/3! + â€¦
compare term by term
True for any analytic Ïˆ with +ve coefficients and starting with squared term.
)

Define xâ‚Š = max(0, x)   positive part
       xâ‚‹ = min(0, -x)  negative part

x = xâ‚Š-xâ‚‹

ğ”¼[exp(sXáµ¢)] = 1 + sğ”¼[Xáµ¢] + ğ”¼[Ïˆ(sXáµ¢)]
            = 1 + ğ”¼[Ïˆ(sXáµ¢)]                     (E[Xáµ¢] = 0)
            â‰¤ 1 + ğ”¼(Ïˆ(s(Xáµ¢)â‚Š) + Ïˆ(-s(Xáµ¢)â‚‹)]
            â‰¤ 1 + E[Ïˆ(s(Xáµ¢)â‚Š) + Â½sÂ²(Xáµ¢)â‚‹Â²]      (Using Ïˆ(x) â‰¤ xÂ²/2 for xâ‰¤ 0)

Assume Xáµ¢ â‰¤ 1

E[exp(sXáµ¢)] â‰¤ 1 + E[Ïˆ(s(Xáµ¢)â‚Š) + Â½sÂ²(Xáµ¢)â‚‹Â²] â‰¤ 1+Ïˆ(s) ... ???

Don't get argument in book

Try again

ğ”¼[exp(sXáµ¢)] â‰¤ 1 + ğ”¼[Ïˆ(sXáµ¢)]
            = 1 + ğ”¼[sÂ²Xáµ¢Â²/2 Ïˆ(sXáµ¢)/(sÂ²Xáµ¢Â²/2)]
            â‰¤ 1 + sÂ²/2 ğ”¼[Xáµ¢Â²] Ïˆ(s)/(sÂ²/2)       (Ïˆ(s)/(sÂ²/2) is monotonically increasing and Xáµ¢â‰¤1)
            = 1 + Ïˆ(s)ğ”¼[Xáµ¢Â²]
            â‰¤ exp(Ïˆ(s)ğ”¼[Xáµ¢Â²])

We had
â„™{S_n-ğ”¼S_n â‰¥ t} â‰¤ exp(-st) âˆ ğ”¼[ exp(s(Xáµ¢-ğ”¼Xáµ¢)) ]

If ğ”¼Xáµ¢ = 0 we have

â„™{S_n â‰¥ t} â‰¤ exp(-st) âˆ ğ”¼[ exp(sXáµ¢) ]
           â‰¤ exp(-st) âˆ exp(Ïˆ(s)ÏƒÂ²)
           â‰¤ exp(nÏˆ(s)ÏƒÂ²-st)

Minimise over s

f(s) = nÏƒÂ²(exp(s)-s-1) - st
f'(s) = nÏƒÂ²(exp(s)-1) - t = 0

exp(s)-1 = t/(nÏƒÂ²)
s = log(1+t/(nÏƒÂ²))

f(s) = nÏƒÂ²(1+t/(nÏƒÂ²)-log(1+t/(nÏƒÂ²)))-tlog(1+t/(nÏƒÂ²))
     = nÏƒÂ²(1+t/nÏƒÂ²-(1+t/(nÏƒÂ²)log(1+t/(nÏƒÂ²))))

âˆ´ Bennett's Inequality
----------------------
ÏƒÂ² = (1/n)âˆ‘Var(Xáµ¢)
Xáµ¢ independent
Xáµ¢ â‰¤ 1 with probability 1.

For any t > 0:

So â„™{âˆ‘Xáµ¢ > t} â‰¤ exp(-nÏƒÂ²h(t/(nÏƒÂ²)))
where h(u) = (1+u)log(1+u)-u

--

RTP h(u) â‰¥ uÂ²/(2+2u/3) for u â‰¥ 0
True at u = 0.

h'(u) = log(1+u)
RHS' = (2u(2+2u/3) - 2uÂ²/3)/(2 + 2u/3)Â²
     = (4u+4uÂ²/3-2uÂ²/3)/(2+2u/3)Â²
     = (4u+2uÂ²/3)/(2+2u/3)Â²
     = 3(12u+2uÂ²)/2(6+2u)Â²
     = 3(6u+uÂ²)/2(3+u)Â²
     = 3u(6+u)/2(3+u)Â²
h'(0) = 0
RHS'(0) = 0
RHS''(u) = 27/(u+3)Â³
h''(u) = 1/(1+u)
h''(u)-RHS''(u) = uÂ²(u+9)/(u+1)/(u+3)Â³

So h(u) is bigger for uâ‰¥0

Bernstein's Inequality
=----------------------
For Îµ>0 and conditions like above:
we now have â„™{(1/n)âˆ‘X_i â‰¥ Îµ} â‰¤ exp(-nÎµÂ²/2(ÏƒÂ²+Îµ/3))

Efron-Stein
-----------

(Towering proprty
ğ”¼[V|E] = ğ”¼[ğ”¼[V|U,W] | W]
)

Xáµ¢ independent
Z = g(Xâ‚,â€¦,X_n)
Var(Z) â‰¤ âˆ‘ğ”¼E[(Z-Eáµ¢Z)Â²]

ğ”¼[XY] = ğ”¼[ğ”¼[XY|Y]] = ğ”¼[Yğ”¼[X|Y]]

Let V = Z-ğ”¼Z
So Var Z = ğ”¼(VÂ²)
Let Váµ¢ = ğ”¼[Z|Xâ‚,â€¦,Xáµ¢]-ğ”¼[Z|Xâ‚,â€¦,Xáµ¢â‚‹â‚]

Eg. n=2:
Vâ‚ = ğ”¼[Z|Xâ‚] - ğ”¼[Z]
Vâ‚‚ = ğ”¼[Z|Xâ‚, Xâ‚‚] - ğ”¼[Z|Xâ‚]
Vâ‚+Vâ‚‚ = Z-ğ”¼[Z] because ğ”¼[Z|Xâ‚, Xâ‚‚] is just f(Xâ‚,Xâ‚‚) as Z no longer random
if we know what Xâ‚ and Xâ‚‚ are.
So we're basically creeping up on V one variable at a time.
So V = âˆ‘Váµ¢

Suppose i>j
ğ”¼Váµ¢Vâ±¼ = ğ”¼ğ”¼[Váµ¢Vâ±¼|Xâ‚,â€¦,Xâ±¼] (because already assuming Xâ‚,â€¦,Xâ±¼ given)
      = ğ”¼[Vâ±¼ğ”¼[Váµ¢|Xâ‚,â€¦,Xâ±¼]] (as Vâ±¼ constant if Xâ‚,â€¦,Xâ±¼ given)

Consider ğ”¼[Váµ¢|Xâ‚,â€¦,Xâ±¼]

Váµ¢ = ğ”¼[Z|Xâ‚,â€¦,Xáµ¢] - ğ”¼[Z|Xâ‚,â€¦,Xáµ¢â‚‹â‚]
ğ”¼[Váµ¢|Xâ‚,â€¦,Xâ±¼] = ğ”¼[ğ”¼[Z|Xâ‚,â€¦,Xáµ¢]|Xâ‚,â€¦,Xâ±¼] - ğ”¼[ğ”¼[Z|Xâ‚,â€¦,Xáµ¢â‚‹â‚]|Xâ‚,â€¦,Xâ±¼]
              = ğ”¼[Z|Xâ‚,â€¦,Xâ±¼] - ğ”¼[Z|Xâ‚,â€¦,Xâ±¼]
              = 0

Var(Z) = ğ”¼[(âˆ‘Váµ¢)Â²]
       = ğ”¼âˆ‘Váµ¢Â² + 2ğ”¼âˆ‘(iâ‰ j) Váµ¢Vâ±¼
       = ğ”¼âˆ‘Váµ¢Â²

(
E[X]Â² â‰¤ E[XÂ²]

ğ”¼[(X-ğ”¼X)Â²] = ğ”¼[XÂ²-2Xğ”¼X+(ğ”¼X)Â²]
           = ğ”¼[XÂ²]-2(ğ”¼X)Â²+(ğ”¼X)Â²]
           = E[XÂ²]-(ğ”¼X)Â² â‰¥ 0
)

Váµ¢Â² = (ğ”¼[Z|Xâ‚,â€¦,Xáµ¢]-ğ”¼[Z|Xâ‚,â€¦,Xáµ¢â‚‹â‚])Â²
    = (ğ”¼[ ğ”¼[Z|Xâ‚,â€¦,X_n]-ğ”¼[Z|Xâ‚,â€¦,Xáµ¢â‚‹â‚,Xáµ¢â‚Šâ‚,â€¦,X_n] | Xâ‚,â€¦,Xáµ¢])Â²
    â‰¤ ğ”¼[ (ğ”¼[Z|Xâ‚,â€¦,X_n]-ğ”¼[Z|Xâ‚,â€¦,Xáµ¢â‚‹â‚,Xáµ¢â‚Šâ‚,â€¦,X_n])Â² | Xâ‚,â€¦,Xáµ¢ ]
    = ğ”¼[ (ğ”¼[Z|Xâ‚,â€¦,X_n]-ğ”¼[Z|Xâ‚,â€¦,Xáµ¢â‚‹â‚,Xáµ¢â‚Šâ‚,â€¦,X_n])Â² | Xâ‚,â€¦,Xáµ¢ ]
    = ğ”¼[ (Z-ğ”¼Z)Â² | Xâ‚,â€¦,Xáµ¢ ]

Define ğ”¼áµ¢Z = ğ”¼[Z|Xâ‚,â€¦,Xáµ¢â‚‹â‚,Xáµ¢â‚Šâ‚,â€¦,X_n]

ğ”¼[Váµ¢Â²] = ğ”¼[(Z-ğ”¼áµ¢Z)Â²]

So Theorem 4
Let Xâ‚',â€¦,X_n' be an independent copy of Xâ‚,â€¦,X_n
Define Záµ¢' = g(Xâ‚,â€¦,Xáµ¢',â€¦,X_n)

Var Z â‰¤ âˆ‘ ğ”¼[(Z-ğ”¼áµ¢Z)Â²]

Efron-Stein Inequality
----------------------

Var(Z) â‰¤ Â½âˆ‘ğ”¼[(Z-Záµ¢')Â²]

Proof:
Var(X) = Â½ ğ”¼[(X-Y)Â²] if X and Y i.i.d.
ğ”¼[(Z-ğ”¼áµ¢Z)Â²] = Â½ ğ”¼áµ¢[(Z-Záµ¢')Â²]

(See conc1.jl for tests)

Intuitively: the total variance is bounded by the expected "one-variable-at-a-time" variation. The factor Â½ comes from doing variation between two copies of Z.

Eg.
X,Y,X',Y' ~ N(0,1)
Z = X+Y
Zâ‚' = X'+Y
Zâ‚‚' = X+Y'

Var(Z) = Var(X+Y) = Var(X)+Var(Y) = 2

ğ”¼(Z-Zâ‚')Â²] = ğ”¼[(X+Y-X'-Y)Â²]
           = ğ”¼[XÂ²+X'Â²]
           = 2
So RHS = 4
2 â‰¤ Â½4 âœ“

Intuition: Sums are least concentrating and bound is met.

---

Var(X) â‰¤ ğ”¼[(X-a)Â²] forall a. (a=ğ”¼X is minimum.)

Define Záµ¢ = gáµ¢(Xâ‚,â€¦,Xáµ¢â‚‹â‚,Xáµ¢â‚Šâ‚,â€¦,X_n)

So ğ”¼áµ¢[(Z-ğ”¼áµ¢Z)Â²] â‰¤ ğ”¼áµ¢[(Z-Záµ¢)Â²]
Which gives, using proof above,

Theorem 6
---------
Var Z â‰¤ âˆ‘ğ”¼[(Z-Záµ¢)Â²]
This is for any measurable gáµ¢

Jackknife Estimate
------------------
Xâ‚,â€¦,X_n i.i.d
Want to estimate Î¸ by Z = f(Xâ‚,â€¦,X_n)
Quality measures: bias ğ”¼Z-Î¸, variance
Jacknife estimate of bias:
(n-1)/n âˆ‘Záµ¢-Z
Záµ¢ some function of Xâ½â±â¾ = (Xâ‚,â€¦,Xáµ¢â‚‹â‚,Xáµ¢â‚Šâ‚,â€¦,X_n)
Jacknife estimate of variance:
âˆ‘(Z-Záµ¢)Â²
sometimes with (n-1)/n out front.

Bounded differences
-------------------
Bounded property:
sup |g(xâ‚,â€¦,x_n)-g(xâ‚,â€¦,xáµ¢â‚‹â‚,xáµ¢',xáµ¢â‚Šâ‚,â€¦,x_n)| â‰¤ cáµ¢
sup over all xâ‚,â€¦,x_n,xáµ¢'

Corollary 1
-----------
Var ZX â‰¤ Â½ âˆ‘cáµ¢Â²

---
ğ’œ collection of subsets of ğ’³
Î¼(A) = â„™{Xâ‚âˆˆA}

Î¼_n(A) = (1/n) âˆ‘ğŸ™_{X_nâˆˆA}

Z = sup (Aâˆˆğ’œ) |Î¼_n(A)-Î¼(A)|

? Sum of n 0-1 variables always â‰¤ 1/(2n) ?

In this case we're looking at sup.

--
Better estimate for Var Zâ€¦
Remember Xáµ¢ are i.i.d.

F some class of real valued functions.
Z = g(Xâ‚, â€¦, X_n) = sup (fâˆˆF) âˆ‘ f(Xâ±¼)

Let's say that for all f âˆˆ F:
ğ”¼[f(Xáµ¢)] = 0, f(x) âˆˆ [-1, 1]

Záµ¢ = sup (fâˆˆF) âˆ‘ (jâ‰ i) f(Xâ±¼)
(i.e. Z misisng ith term)

Let fÌ‚ achieve supremum so
Z = âˆ‘fÌ‚(Xáµ¢)

âˆ‘f(Xáµ¢) â‰¤ âˆ‘fÌ‚(Xáµ¢) defá¹‡ of fÌ‚
âˆ‘(jâ‰ i) f(Xáµ¢) â‰¤ âˆ‘fÌ‚áµ¢(Xáµ¢) defá¹‡ of fÌ‚

Also define fÌ‚áµ¢ such that Záµ¢ = âˆ‘(jâ‰ i) fÌ‚áµ¢(Xâ±¼)

By defá¹‡ Z-Záµ¢ = âˆ‘(jâ‰ i)(fÌ‚(Xâ±¼)-fÌ‚áµ¢(Xâ±¼)) + fÌ‚(Xáµ¢)
               \__________________/
                  â‰¤0 by defá¹‡ of fÌ‚áµ¢

By defá¹‡ Z-Záµ¢ = âˆ‘(fÌ‚(Xâ±¼)-fÌ‚áµ¢(Xâ±¼)) - fÌ‚(Xáµ¢) + fÌ‚áµ¢(Xáµ¢) + fÌ‚(Xáµ¢)
             = âˆ‘(fÌ‚(Xâ±¼)-fÌ‚áµ¢(Xâ±¼)) + fÌ‚áµ¢(Xáµ¢)
               \_____________/
                â‰¥0 by defá¹‡ of fÌ‚

Combining gives:

fÌ‚áµ¢(Xáµ¢) â‰¤ Z-Záµ¢ â‰¤ fÌ‚(Xáµ¢)

âˆ´ âˆ‘(Z-Záµ¢) â‰¤ Z (By defá¹‡ Z = âˆ‘fÌ‚(Xáµ¢))

(I don't know what it means for fÌ‚áµ¢ and Xáµ¢ to be independent.
By defá¹‡ ğ”¼áµ¢[fÌ‚áµ¢(Xáµ¢)] = ğ”¼[ fÌ‚áµ¢(Xáµ¢) | Xâ‚,â€¦,Xáµ¢â‚‹â‚,Xáµ¢â‚Šâ‚,â€¦,X_n ]
??? ğ”¼áµ¢[fÌ‚áµ¢(Xáµ¢)] = 0 ???
Makes perfect sense. fÌ‚áµ¢ is a random variable selected based on
Xâ‚,â€¦XÌ‚áµ¢,â€¦,X_n
)

ğ”¼[fÌ‚áµ¢(Xáµ¢)] = 0

???
(Z-Záµ¢)Â² - fÌ‚áµ¢Â²(Xáµ¢) = (Z-Záµ¢+fÌ‚áµ¢(Xáµ¢))(Z-Záµ¢-fÌ‚áµ¢(Xáµ¢))
                  â‰¤ 2(Z-záµ¢+fÌ‚áµ¢(Xáµ¢))
??? XXX Come back to this ???
See conc3.jl for counterexample. This is wrong.

ğ”¼[ âˆ‘(Z-Záµ¢)Â² ] â‰¤ ğ”¼[ âˆ‘fÌ‚áµ¢(Xáµ¢)Â² + 2(Z-Záµ¢) + 2fÌ‚áµ¢(Xáµ¢) ]
              â‰¤ n sup(fâˆˆF) ğ”¼[f(Xâ‚)Â²]    (trivial 'cos sup)
                + 2ğ”¼[Z]                 ('cos âˆ‘ Z-Záµ¢ â‰¤ Z)
                + 0                     ('cos ğ”¼fÌ‚áµ¢(Xáµ¢) = 0)
So

Var Z â‰¤ n sup(fâˆˆF) ğ”¼[f(Xâ‚)Â²] + 2ğ”¼[Z]

--

Minimum of Empirical Loss
-------------------------

F a class of {0,1}- values functions on X.
Assume F finite.
i.i.d D_n = (âŒ©Xáµ¢,Yáµ¢âŒª)_{iâ‰¤n}
âŒ©Xáµ¢,Yáµ¢âŒª take values in XÃ—{0,1}
Define empirical loss:
L_n(f) = (1/n) âˆ‘ â„“(f(Xáµ¢), Yáµ¢) with â„“(y,y') = ğŸ™_{yâ‰ y'}

Often one needs:

LÌ‚ = inf_{fâˆˆF} L_n(f)

E-S gives Var(LÌ‚) â‰¤ 1/(2n)

Z - nLÌ‚ and Záµ¢' as above.
Define Záµ¢' = min_{fâˆˆF} [ â„“(f(Xáµ¢), Yáµ¢) + â„“(f(Xáµ¢'), Yáµ¢') ]
âŒ©Xáµ¢', Yáµ¢'âŒª same distribution as âŒ©Xáµ¢,Yáµ¢âŒª but independent.

Var X â‰¤ Â½ âˆ‘ ğ”¼[(Z-Záµ¢')Â²] = âˆ‘ ğ”¼[(Z-Záµ¢')Â²ğŸ™_{Záµ¢'>Z}]
??? Where does that last bit come from ???
OK, switching do document (4) now as I don't trust (1)

Uniform Deviations
------------------
Xâ‚,,â€¦,X_n in ğ’³ i.i.d
ğ’œ is collection of subsets of ğ’³
Î¼ is distribution of Xâ‚ so Î¼(A) = â„™{Xâ‚âˆˆA}
Î¼_n is empirical distribution

Î¼_n(A) = (1/n) âˆ‘ğŸ™{Xáµ¢âˆˆAáµ¢}

We want Z = sup(Aâˆˆğ’œ)|Î¼_n(A)-Î¼(A)|
If lim(nâ†’âˆ)ğ”¼Z = 0 for every Î¼ then ğ’œ is called a uniform Glivenko-Cantelli class.
Changing one Xáµ¢ changes Z by at most 1/n.
So Var(Z) â‰¤ 1/(2n)
--
Better estimate:

Let â„± be some class of functions.
Define Z = g(Xâ‚,â€¦,X_n) = sup(fâˆˆâ„±) âˆ‘f(Xâ±¼)

Write Efron-Stein as

Var(Z) â‰¤ Â½ âˆ‘ğ”¼[(Z-Záµ¢')Â²] = âˆ‘ğ”¼[(Z-Záµ¢')Â²ğŸ™{Záµ¢'<Z}]
Let f* be the actual supremum (which is a function of the Xáµ¢)
So Z = âˆ‘f*(Xâ±¼)

(Z-Záµ¢')Â²ğŸ™{Záµ¢'<Z} â‰¤ (f*(Xáµ¢)-f*(Xáµ¢'))Â²

(Z-Záµ¢')Â² â‰¤ ğ”¼[sup(fâˆˆâ„±) âˆ‘ ((f(Xáµ¢)-f(Xáµ¢'))Â² ]
